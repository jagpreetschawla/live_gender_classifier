{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading images and splitting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "X, Y = [], []\n",
    "dir_ = {'data/male/': 1, 'data/female/': 0}\n",
    "for d, y in dir_.items():\n",
    "    for i in os.listdir(d):\n",
    "        f = os.path.join(d, i)\n",
    "        if os.path.isfile(f) and i.endswith(\".png\"):\n",
    "            img = imread(f)/255.0\n",
    "            X.append(img)\n",
    "            Y.append(y)\n",
    "\n",
    "tmp = list(zip(X, Y))\n",
    "random.shuffle(tmp)\n",
    "X, Y = zip(*tmp)\n",
    "test_size = int(0.05 * len(X))\n",
    "X_test, Y_test = np.array(X[-test_size:], dtype=np.float32), np.array(Y[-test_size:])\n",
    "X_train, Y_train = np.array(X[0:-test_size], dtype=np.float32), np.array(Y[0:-test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(features, labels, mode):\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 25*25])\n",
    "    layer_1 = tf.layers.dense(inputs=input_layer, units=200, activation=tf.nn.relu)\n",
    "    layer_output = tf.layers.dense(inputs=layer_1, units=2)\n",
    "    predictions = { \n",
    "        \"prob\": tf.nn.softmax(layer_output), \n",
    "        \"output\": tf.argmax(input=layer_output, axis=1) \n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, \n",
    "                                          export_outputs={\n",
    "                                              \"prediction\": tf.estimator.export.ClassificationOutput(\n",
    "                                                  scores=predictions[\"prob\"]\n",
    "                                              )\n",
    "                                          })\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    # out_clipped = tf.clip_by_value(layer_output,1e-10,1e10)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=layer_output)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "        labels=labels, predictions=predictions[\"output\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1103b23d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mf_classifier', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "mf_classifier = tf.estimator.Estimator(\n",
    "    model_fn=classifier_model_fn, model_dir=\"/tmp/mf_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/mf_classifier/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.9710674, step = 1\n",
      "INFO:tensorflow:global_step/sec: 488.25\n",
      "INFO:tensorflow:loss = 0.51167893, step = 101 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.664\n",
      "INFO:tensorflow:loss = 0.4731263, step = 201 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.602\n",
      "INFO:tensorflow:loss = 0.42376804, step = 301 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.54\n",
      "INFO:tensorflow:loss = 0.38365522, step = 401 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.894\n",
      "INFO:tensorflow:loss = 0.47763625, step = 501 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.553\n",
      "INFO:tensorflow:loss = 0.27807254, step = 601 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 511.527\n",
      "INFO:tensorflow:loss = 0.33722907, step = 701 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.513\n",
      "INFO:tensorflow:loss = 0.3185409, step = 801 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 511.878\n",
      "INFO:tensorflow:loss = 0.27821222, step = 901 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.096\n",
      "INFO:tensorflow:loss = 0.2254455, step = 1001 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 503.714\n",
      "INFO:tensorflow:loss = 0.31699464, step = 1101 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.018\n",
      "INFO:tensorflow:loss = 0.25101697, step = 1201 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.529\n",
      "INFO:tensorflow:loss = 0.22330646, step = 1301 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.501\n",
      "INFO:tensorflow:loss = 0.27344266, step = 1401 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.902\n",
      "INFO:tensorflow:loss = 0.44017592, step = 1501 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 519.41\n",
      "INFO:tensorflow:loss = 0.22523713, step = 1601 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.777\n",
      "INFO:tensorflow:loss = 0.25565872, step = 1701 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.765\n",
      "INFO:tensorflow:loss = 0.37892163, step = 1801 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.79\n",
      "INFO:tensorflow:loss = 0.241725, step = 1901 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 504.969\n",
      "INFO:tensorflow:loss = 0.22324698, step = 2001 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.179\n",
      "INFO:tensorflow:loss = 0.1830905, step = 2101 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.697\n",
      "INFO:tensorflow:loss = 0.22103618, step = 2201 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 538.92\n",
      "INFO:tensorflow:loss = 0.1792651, step = 2301 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.89\n",
      "INFO:tensorflow:loss = 0.17319442, step = 2401 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.459\n",
      "INFO:tensorflow:loss = 0.19084091, step = 2501 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.557\n",
      "INFO:tensorflow:loss = 0.16579942, step = 2601 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.132\n",
      "INFO:tensorflow:loss = 0.19164468, step = 2701 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.87\n",
      "INFO:tensorflow:loss = 0.26043576, step = 2801 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.545\n",
      "INFO:tensorflow:loss = 0.14437138, step = 2901 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.528\n",
      "INFO:tensorflow:loss = 0.13953233, step = 3001 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.176\n",
      "INFO:tensorflow:loss = 0.11560267, step = 3101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.463\n",
      "INFO:tensorflow:loss = 0.14169109, step = 3201 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 556.204\n",
      "INFO:tensorflow:loss = 0.111834705, step = 3301 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.825\n",
      "INFO:tensorflow:loss = 0.1692334, step = 3401 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.687\n",
      "INFO:tensorflow:loss = 0.14024764, step = 3501 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.787\n",
      "INFO:tensorflow:loss = 0.17051727, step = 3601 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.205\n",
      "INFO:tensorflow:loss = 0.10640111, step = 3701 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.96\n",
      "INFO:tensorflow:loss = 0.0563656, step = 3801 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 519.068\n",
      "INFO:tensorflow:loss = 0.07922629, step = 3901 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.478\n",
      "INFO:tensorflow:loss = 0.118362516, step = 4001 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.803\n",
      "INFO:tensorflow:loss = 0.09632981, step = 4101 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.992\n",
      "INFO:tensorflow:loss = 0.08224433, step = 4201 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.961\n",
      "INFO:tensorflow:loss = 0.066839784, step = 4301 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 539.284\n",
      "INFO:tensorflow:loss = 0.10760726, step = 4401 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.383\n",
      "INFO:tensorflow:loss = 0.09596483, step = 4501 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 516.411\n",
      "INFO:tensorflow:loss = 0.12632303, step = 4601 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.63\n",
      "INFO:tensorflow:loss = 0.079159886, step = 4701 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.63\n",
      "INFO:tensorflow:loss = 0.0746788, step = 4801 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.927\n",
      "INFO:tensorflow:loss = 0.05290654, step = 4901 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.659\n",
      "INFO:tensorflow:loss = 0.15406209, step = 5001 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 509.181\n",
      "INFO:tensorflow:loss = 0.05664717, step = 5101 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.123\n",
      "INFO:tensorflow:loss = 0.21773823, step = 5201 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.151\n",
      "INFO:tensorflow:loss = 0.082100436, step = 5301 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.901\n",
      "INFO:tensorflow:loss = 0.042868394, step = 5401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.332\n",
      "INFO:tensorflow:loss = 0.03426485, step = 5501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.873\n",
      "INFO:tensorflow:loss = 0.04785572, step = 5601 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 543\n",
      "INFO:tensorflow:loss = 0.03231541, step = 5701 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 540.047\n",
      "INFO:tensorflow:loss = 0.058739, step = 5801 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.329\n",
      "INFO:tensorflow:loss = 0.05879399, step = 5901 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.192\n",
      "INFO:tensorflow:loss = 0.08411306, step = 6001 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.502\n",
      "INFO:tensorflow:loss = 0.032148506, step = 6101 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.923\n",
      "INFO:tensorflow:loss = 0.087431245, step = 6201 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.971\n",
      "INFO:tensorflow:loss = 0.030565085, step = 6301 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 508.285\n",
      "INFO:tensorflow:loss = 0.045057494, step = 6401 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.626\n",
      "INFO:tensorflow:loss = 0.024745965, step = 6501 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 541.729\n",
      "INFO:tensorflow:loss = 0.049054313, step = 6601 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.955\n",
      "INFO:tensorflow:loss = 0.024063528, step = 6701 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.836\n",
      "INFO:tensorflow:loss = 0.022574324, step = 6801 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.759\n",
      "INFO:tensorflow:loss = 0.021357404, step = 6901 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.048\n",
      "INFO:tensorflow:loss = 0.044500984, step = 7001 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.691\n",
      "INFO:tensorflow:loss = 0.026084943, step = 7101 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.955\n",
      "INFO:tensorflow:loss = 0.025657615, step = 7201 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.284\n",
      "INFO:tensorflow:loss = 0.029722003, step = 7301 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.131\n",
      "INFO:tensorflow:loss = 0.029730322, step = 7401 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 538.083\n",
      "INFO:tensorflow:loss = 0.06570093, step = 7501 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 540.088\n",
      "INFO:tensorflow:loss = 0.017175836, step = 7601 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.425\n",
      "INFO:tensorflow:loss = 0.010321136, step = 7701 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.587\n",
      "INFO:tensorflow:loss = 0.01702114, step = 7801 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 535.739\n",
      "INFO:tensorflow:loss = 0.025083141, step = 7901 (0.187 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 8000 into /tmp/mf_classifier/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00807002.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1103b26d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=Y_train,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "mf_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-21-19:38:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mf_classifier/model.ckpt-8000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-21-19:38:59\n",
      "INFO:tensorflow:Saving dict for global step 8000: accuracy = 0.8969697, global_step = 8000, loss = 0.30619115\n",
      "{'loss': 0.30619115, 'global_step': 8000, 'accuracy': 0.8969697}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test},\n",
    "    y=Y_test,\n",
    "    shuffle=False)\n",
    "eval_results = mf_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mf_classifier/model.ckpt-8000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jp/python/ml/ml_notebooks/venv/lib/python2.7/site-packages/skimage/io/_plugins/matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n",
      "/Users/jp/python/ml/ml_notebooks/venv/lib/python2.7/site-packages/matplotlib/axes/_base.py:1400: MatplotlibDeprecationWarning: The 'box-forced' keyword argument is deprecated since 2.2.\n",
      "  \" since 2.2.\", cbook.mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the model\n",
    "check_index=110\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test[check_index]}, shuffle=False)\n",
    "predict_results = mf_classifier.predict(input_fn=predict_input_fn, predict_keys=[\"output\"])\n",
    "imshow(X_test[check_index])\n",
    "print([\"male\" if i[\"output\"] else \"female\" for i in predict_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting our model:\n",
    "Exported model will be deployed using tensorflow-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'prediction']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mf_classifier/model.ckpt-8000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./models/temp-1524339561/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./models/1524339561'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_classifier.export_savedmodel(\"./models/\",\n",
    "                                tf.estimator.export.build_parsing_serving_input_receiver_fn({\n",
    "                                    \"x\": tf.FixedLenFeature(shape=[25,25], dtype=tf.float32)\n",
    "                                }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
